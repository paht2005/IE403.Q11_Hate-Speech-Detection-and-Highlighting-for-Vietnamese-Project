% 1. ViHSD (LREC)
@InProceedings{10.1007/978-3-030-79457-6_35,
author="Luu, Son T.
and Nguyen, Kiet Van
and Nguyen, Ngan Luu-Thuy",
editor="Fujita, Hamido
and Selamat, Ali
and Lin, Jerry Chun-Wei
and Ali, Moonis",
title="A Large-Scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts",
booktitle="Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="415--426",
abstract="In recent years, Vietnam witnesses the mass development of social network users on different social platforms such as Facebook, Youtube, Instagram, and Tiktok. On social media, hate speech has become a critical problem for social network users. To solve this problem, we introduce the ViHSD - a human-annotated dataset for automatically detecting hate speech on the social network. This dataset contains over 30,000 comments, each comment in the dataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, we introduce the data creation process for annotating and evaluating the quality of the dataset. Finally, we evaluate the dataset by deep learning and transformer models.",
isbn="978-3-030-79457-6"
}

% 2. ViTHSD (arXiv)
@inproceedings{vithsd,
  author    = {Vo, Cuong Nhat and Huynh, Khanh Bao and Luu, Son T. and Trong-Hop, D.},
  title     = {ViTHSD: Exploiting Hatred by Targets for Hate Speech Detection on Vietnamese Social Media Texts},
  booktitle = {Proceedings of the Journal of Computational Social Science},
  year      = {2025},
  doi       = {10.1007/s42001-024-00348-6},
  url       = {https://doi.org/10.1007/s42001-024-00348-6}
}

% 3. PhoBERT (Findings EMNLP 2020)
@inproceedings{nguyen2020phobert,
  title     = "{P}ho{BERT}: Pre-trained Language Models for {V}ietnamese",
  author    = "Nguyen, Dat Quoc and Nguyen, Anh Tuan",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
  year      = "2020",
  address   = "Online",
  publisher = "Association for Computational Linguistics",
  pages     = "1037--1042"
}

% 4. Qwen2.5 technical report (arXiv)
@misc{Yang2025Qwen25,
  author       = {Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
  title        = {Qwen2.5 Technical Report},
  howpublished = {arXiv preprint arXiv:2412.15115},
  year         = {2025},
  doi          = {10.48550/arXiv.2412.15115},
  url          = {https://arxiv.org/abs/2412.15115}
}


% 5. Scaling Instruction-Finetuned LMs (arXiv)
@misc{chung2022scaling,
author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and
          Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and
          Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and
          Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and
          Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and
          Robinson, Kevin and Valter, Dasha and Narang, Sharan and
          Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and
          Dai, Andrew M. and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and
          Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and
          Le, Quoc V. and Wei, Jason},
          title        = {Scaling Instruction-Finetuned Language Models},
          howpublished = {arXiv preprint arXiv:2210.11416},
          year         = {2022},
          url          = {https://arxiv.org/abs/2210.11416},
}


% 6. Gemini 1.5 (arXiv)
@misc{google2025gemini2flash,
  author = {Google DeepMind},
  title = {Gemini 2.0 Flash: Efficient Multimodal Reasoning Model},
  howpublished = {Google AI Studio Documentation},
  year = {2025},
  note = {Accessed via Google Developers and Gemini API docs},
  url = {https://ai.google.dev/gemini-api/docs/models/experimental-models}
}

% 7. Llama 2 (arXiv)
@misc{llama,
  author       = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Canton Ferrer, Cristian and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Singh Koura, Punit and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  howpublished = {arXiv preprint arXiv:2307.09288},
  year         = {2023},
  url          = {https://arxiv.org/abs/2307.09288}
}


% 8. Gemma (arXiv)
@misc{Mesnard2024Gemma,
  author       = {Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, Léonard and Sessa, Pier Giuseppe and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L. Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy and others},
  title        = {Gemma: Open Models Based on Gemini Research and Technology},
  howpublished = {arXiv preprint arXiv:2403.08295},
  year         = {2024},
  url          = {https://arxiv.org/abs/2403.08295}
}


% 9. Word2Vec (ICLR workshop)
@inproceedings{Mikolov2013Efficient,
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations (ICLR 2013), Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781}
}


% 10. FastText / Subword (TACL)
@article{Bojanowski2017Enriching,
  author  = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  title   = {Enriching Word Vectors with Subword Information},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {5},
  pages   = {135--146},
  year    = {2017},
  doi     = {10.1162/tacl_a_00051},
  url     = {https://aclanthology.org/Q17-1010/}
}


% 11. HARE (Findings EMNLP 2023)
@inproceedings{Yang2023HARE,
  author    = {Yang, Yongjin and Kim, Joonkee and Kim, Yujin and Ho, Namgyu and Thorne, James and Yun, Se-Young},
  title     = {HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages     = {5490--5505},
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.findings-emnlp.365},
  url       = {https://aclanthology.org/2023.findings-emnlp.365/}
}


% 12. Chain-of-Thought (NeurIPS 2022)
@misc{Wei2022ChainOfThought,
  author       = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year         = {2022},
  howpublished = {arXiv preprint arXiv:2201.11903},
  doi          = {10.48550/arXiv.2201.11903},
  url          = {https://arxiv.org/abs/2201.11903}
}


% 13. QLoRA (NeurIPS 2023)
@misc{Dettmers2023QLoRA,
  author       = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title        = {QLoRA: Efficient Finetuning of Quantized LLMs},
  year         = {2023},
  howpublished = {arXiv preprint arXiv:2305.14314},
  doi          = {10.48550/arXiv.2305.14314},
  url          = {https://arxiv.org/abs/2305.14314}
}

% 14. SemEval-2014 Task 4 (ABSA)
@inproceedings{Pontiki2014SemEval4,
  author    = {Pontiki, Maria and Galanis, Dimitris and Pavlopoulos, John and
               Papageorgiou, Harris and Androutsopoulos, Ion and Manandhar, Suresh},
  title     = {SemEval-2014 Task 4: Aspect Based Sentiment Analysis},
  booktitle = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
  pages     = {27--35},
  year      = {2014},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/S14-2004},
  url       = {https://aclanthology.org/S14-2004/}
}


% 15. Transformer (Vaswani et al. 2017, NeurIPS)
@inproceedings{Vaswani2017Attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and
               Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  title     = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS 2017)},
  pages     = {6000--6010},
  year      = {2017},
  publisher = {Curran Associates, Inc.},
  url       = {https://papers.nips.cc/paper/7181-attention-is-all-you-need},
}

% 16. BERT (NAACL 2019)
@inproceedings{Devlin2019BERT,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages     = {4171--4186},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/N19-1423},
  url       = {https://www.aclanthology.org/N19-1423/}
}

